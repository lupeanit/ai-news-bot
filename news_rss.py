import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime
import pyshorteners # URL ë‹¨ì¶• ë„êµ¬ ì¶”ê°€

def get_google_news_rss(keyword):
    rss_url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
    s = pyshorteners.Shortener() # ë‹¨ì¶•ê¸° ì‹¤í–‰
    
    try:
        response = requests.get(rss_url)
        soup = BeautifulSoup(response.content, features="xml")
        
        items = soup.find_all('item')
        news_data = []
        
        print("ğŸ”— ì£¼ì†Œ ë‹¨ì¶• ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        
        for i, item in enumerate(items[:10], 1): # ë‹¨ì¶• ì†ë„ë¥¼ ìœ„í•´ 10ê°œë§Œ ì¶”ì²œ
            title = item.title.text
            long_url = item.link.text
            
            # TinyURL ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸¸ê³  ë³µì¡í•œ ì£¼ì†Œë¥¼ ì§§ê²Œ ì¤„ì„
            try:
                short_url = s.tinyurl.short(long_url)
            except:
                short_url = long_url # ì—ëŸ¬ ë°œìƒ ì‹œ ì›ë˜ ì£¼ì†Œ ì‚¬ìš©
            
            pub_date = item.pubDate.text
            news_data.append([i, pub_date, title, short_url])

        filename = "Cloud_AI_News_Short.csv"
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ë²ˆí˜¸', 'ë°œí–‰ì‹œê°„', 'ë‰´ìŠ¤ì œëª©', 'ë‹¨ì¶•ë§í¬'])
            writer.writerows(news_data)

        print(f"âœ… ë‹¨ì¶• ì™„ë£Œ! '{filename}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    get_google_news_rss("ì¸ê³µì§€ëŠ¥")