import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime # ë‚ ì§œë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì´ë¯¸ ì„í¬íŠ¸ë˜ì–´ ìˆì„ ê±°ì˜ˆìš”
import pyshorteners

def get_google_news_rss(keyword):
    rss_url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
    s = pyshorteners.Shortener()
    
    try:
        response = requests.get(rss_url)
        soup = BeautifulSoup(response.content, features="xml")
        
        items = soup.find_all('item')
        news_data = []
        
        print("ğŸ”— ì£¼ì†Œ ë‹¨ì¶• ë° ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        for i, item in enumerate(items[:10], 1):
            title = item.title.text
            long_url = item.link.text
            try:
                short_url = s.tinyurl.short(long_url)
            except:
                short_url = long_url
            
            pub_date = item.pubDate.text
            news_data.append([i, pub_date, title, short_url])

        # --- [íŒŒì¼ëª…ì— ë‚ ì§œ ë„£ê¸° í•µì‹¬ ë¶€ë¶„] ---
        # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ '2026-01-27' ê°™ì€ í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        today_date = datetime.now().strftime('%Y-%m-%d')
        filename = f"AI_ë‰´ìŠ¤_{today_date}.csv" 
        # -----------------------------------

        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ë²ˆí˜¸', 'ë°œí–‰ì‹œê°„', 'ë‰´ìŠ¤ì œëª©', 'ë‹¨ì¶•ë§í¬'])
            writer.writerows(news_data)

        print(f"âœ… ì™„ë£Œ! '{filename}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    get_google_news_rss("ì¸ê³µì§€ëŠ¥")
